# Overview and pipeline
The approach below details a classification pipeline to predict whether a
 particular post (title + body) on StackOverflow has been marked as off-topic
 . 
 
 The pipeline consists of 3 key stages:
 - Pre-processing
 - Model fitting
 - Model evaluation
  
# Preprocessing Overview
The preprocessing pipeline comprises of two key steps
 - Text parsing of the post's title and body
 - Calculating metadata features about the post's title and body 

The text parsing and cleaning portion consists of two stages:
## Light Cleaning
    - The light cleaning portion of the text parsing removes errata from the
     text but does adjust punctuation, create word lemmas, or remove stop words.
    - This version of the parsed text is used for the metadata details
     calculation and any NLP NN models that have been trained on embeddings
      for punctuation and for sequences of stop words
    - In particular, the BERT model from Google is trained on a corpus that
     includes such words and specifically uses them to understand sentence
     structure
    - Specific steps in light cleaning include:
        - Converting to lowercase -> capitalization is often a function of
         word placement in a sentence and represents the same word as its
          lower case counterpart
            - One nuance (not implemented, but may be useful with more
             analysis) may be to not convert UPPERCASE words as the represent
             acronyms distinct from the lower case counterpart (OR=operations
             Research vs or, ZIP as part of ZIP CODE vs zip as in the
             compressed file format)
        - Stripping HTML tags -> purely display information added by the
         website not the author
        - Converting URLs -> URLs are most often unique per post and per
         specific issue that the use is facing. URLs are often dynamically
         generated by the website, and are seldom repeated. However, the
         presence or absence of a link in a post is valuable and we should
         keep a dedicated word for URLs
        - Converting accents -> Diacritics in English (the language in the
         StackOverflow corpus) have a fairly marginal status. Words may be
         spelled with or without accents (cliche or naive) without changing
         their meaning
        - Expanding contractions -> Contractions in English contain the same
         information as their expanded counterparts. Many English writers
         will use the contraction and expanded form interchangeably.
        - Removing multiple spaces -> Multiple spaces contain no
         grammatical or interpretive meaning in English. In addition, the
         StackOverflow corpus contains significant amount of whitespace due
         to coding language requirements. It is important to tell the model
         that spaces in code are not indicative of any additional meaning
         beyond a syntactic requirement.
        - Considered converting emoji to a standard token because they are
         heavily person dependent on the exact emoji used. StackOverflow
         discourages emoji use on its platform so it is less critical to
         remove (low frequency in text). However, on a different set of text
         such as Twitter or SMS/WhatsApp/Instagram messages where emojis
         are more prevalent, would be useful to standardize them to an
         emoji token.
          
## Heavy Cleaning
    - The heavy cleaning portion of the text parsing standardizes individual
     words (more specifically tokens) into their base form and keeps only the
     essential portions of each text
    - This version of the parsed text is used for training models that cannot
     embed relationships across word families (TF-IDF, Bag of Words)
    - Specific steps in heavy cleaning include:
        - Removing punctuation: the presence / absence of punctuation is
         already captured in text metadata. In addition, multiple UTF-8 codes
          may represent the same character (single quote, double quote, comma
          ).
        - Lemmatizing text: multiple distinct words contain very similar
         meaning (stop, stopped, stops, stopping) and models should treat
         them as very similar in meaning. Converts all words to their base
          form (singular, present tense, non conjugated)
        - Removing stopwords: Words with high frequency of occurrence (and
        , is, I, she, it, or) often don't provide much additional meaning to
         a sentence so they should be removed from the text.

## Metadata
The calculated metadata features are intended to incorporate language based
information that is not necessarily the words themselves into the predictive
model. These features guide the model and help it explore non-token based
features of the language space.

The metadata features are calculated on the lightly cleaned (see above) post
 titles and bodies. The lightly cleaned version only removed errata from the
  text, but does not substantially alter the words and paragraphs themselves.
  
The following metadata features are computed:
- Number of code blocks in the post body
    - Posts with examples and code samples are more likely on topic
- Number of words in code blocks in the post body
    - Posts with shorter code samples are more likely on topic (if post
     has code, prefer concise examples, not long drawn out explanations)
- Number of sentences in the post body (sentence defined by punctuation)
    - Longer sentences may indicate more descriptive explanations
- Number of words in the title / body
    - More words in body correlate with more detailed descriptions, more
     descriptive error messages and behaviors and existing tried solutions
- Number of characters in the title / body
    - Similar explanation to number of words, but slightly differently encoded
    - Number of characters is controlled for URLs which are arbitrary distance
- Number of punctuation characters in the title / body
    - Posts with distinct punctuation (no run on sentences, proper commas) may
    be indicative of more thought and effort in the post
- Mean/Median/Max word length
    - Word lengths that are longer may correspond to more detailed and
     descriptive explanations
- Percent of words with meaning
    - After excluding stop words (am, on, it, etc.) from post, what percent
     of words are remaining. Posts with more "content" and less filler words
     may be indicative of detailed and specific code questions instead of
     generalities and vagueness

# Models

## Train / Test split
To generalize solutions to new posts, the pipeline separates the posts into
two distinct sets, a training set and a testing set. Because both classes
are balanced at 50% of all posts and there exists no known temporal
relationship across posts, we can randomly sample into the training and
testing set. 
   
The model is trained on the set of posts (titles and post body) in the
training set, and is validated by scoring posts on the unseen to the model
test set. As long as the test posts are drawn from the same overall pool of
posts as the training set, the model should generalize to new posts.

## TF-IDF
The specific method used to distinguish off-topic versus acceptable posts is
known as `TF-IDF`, which stands for Term Frequency - Inverse Document
Frequency. The tf-idf value of a word increases proportionally to how
frequent a word appears in a document relative to the words's frequency in
the overall set of documents. This method is often used for text-based
recommendation systems and serves as a useful baseline to compare more
advanced Neural Network based algorithms. 
      
The pipeline calculates the tf-idf score for all words found in the (cleaned) 
documents in the training set. These weightings are piped into a
LogisticRegression classifier that attempts to distinguish the off-topic
posts from the acceptable posts. Optionally, the metadata features may also
be fed into the LogisticRegression classifier.
   
Posts in the test set are shown to the pre-trained classifier and are scored
 with the model's "acceptability score" between 0 and 1. Note that the pre
 -trained classifier (both the LogisticRegression and the TF-IDF weightings) has
  not seen these posts before.
  
Because it does not require any GPUs for training, TF-IDF provides an
acceptably complex baseline classifier for this task. It performs well at
distinguishing posts between the classes, is quick to train, and
straightforward to assign scores to new posts

## Bert Transformer
One of the main drawback of the TF-IDF approach is its inability to look at
more detailed syntactic patterns in text. It purely focuses on relative
frequencies and word counts. However, the exact placement of words within a
sentence also carry significant semantic meaning. 

In addition, the TF-IDF model was trained solely on the set of StackOverflow
 posts from the training set. This corpus of documents is comprehensive, but
  not as comprehensive as other sets of text in the English language. A model
   that has been pre-trained on a more general English language corpus (for
    example, Wikipedia) would generalize syntactic patterns better than
     purely StackOverflow posts. 
     
The above framework (pre-training a model on a general corpus, and fine
 tuning it's final layers based on the specific set of texts) is known as
  transfer learning. This approach would learn the English language's general
   patterns and fine tune them to the specifics of StackOverflow post behavior
   
Therefore, a Transformer based model such as Google's BERT mitigates the most
 significant disadvantages of the TF-IDF model. However, this approach is
  significantly more computationally expensive (requires GPUs, lots of memory
  ) and takes much longer to fine tune to the StackOverflow corpus than TF-IDF.

# Model Evaluation Metrics
To evaluate model performance, I used a comprehensive set of evaluation
 metrics. The metrics are listed below:
 - ROC Curve (and AUC)
 - Accuracy
 - Log Loss

## TF-IDF Model Scores
For the TF-IDF model, the above mentioned scores are reported below:
- AUC: 0.86
- Accuracy: 0.78 
- Log-Loss: 0.46 (predicting 0.5 for everything = 0.693, lower is better)

In all 3 metrics, the TF-IDF model performs considerably better than random
and would provide valuable guidance to the poster regarding off-topic likelihood

## AUC
If I would pick a single metric, I would select AUC, which best handles
performance across a number of models. The AUC score represents the
probability that the classifier ranks a randomly chosen positive example
above a randomly chosen negative example. In addition, it generalizes to
the imbalanced and multi-class problems which are more realistic of the
true StackOverflow dataset. Submitted posts are not a 50/50 split between
good and bad, and there are multiple reasons a post may be considered bad
 (off-topic, spam, duplicate).

## Accuracy
However, AUC may be somewhat hard for a non-data scientist to intuitively
grasp (certainly possible but not immediately intuitive). To combat this
challenge, I report the accuracy score as well. Accuracy is a great metric
for this particular challenge because of the balanced classes in the
dataset. It measures what percentage of predictions the model gets correct
, which is the simplest metric of model performance. However, accuracy comes
 with a number of caveats for many data sets. With imbalanced classes
 , accuracy is not as useful of a metric
because a model can perform artificially well by predicting the majority
class for nearly all predictions. This phenomenon does not indicate true
 learning.
 
## AUC versus Accuracy
AUC mitigates this risk by using a "cutoff" score to score models. All
 results above the cutoff are predicted as a 1 and all results below the
 cutoff are predicted as a 0. The intermediate states of this graph will
 plot True Positive and False Positive scores, indicating how good the
 model is at discriminating between each class. Especially for models that
 predict on highly imbalanced data, comparing true and false positive
 rates will create a model that truly learns to distinguish the classes.
 
## Log Loss
In addition to AUC and accuracy, I report log-loss as a way to distinguish how 
confident (or uncertain) the model is in its predictions. A model should be
 penalized for being confident about an incorrect prediction. In other words
 , a model should be rewarded for being more confident (scores closer to 1 or 0
 ) in its correct
predictions than hedging its predictions in the chance it may be wrong. It is
 better to be a little bit wrong about a prediction than completely wrong. If
the model is extremely confident that a post is good (class = 0), then it
should give a score close to 0. If the model is truly unsure of a particular
 post, 
then it should be encouraged to give a score of close to 0.5 rather than
randomly guessing and hoping the guess will be correct. 

## Business cost of False Positive vs False Negative
Finally, I would highlight that all three of the metrics above (AUC, Accuracy
, and Log Loss) are purely guiding principles for evaluating models. The best, 
and cost function will explicitly weigh the cost of mis-prediction in either
direction (predict good, actual bad vs predict bad, actual good) as defined
by the business objective. For example, the StackOverflow team may want to have 
an extra clean platform and preemptively remove any potentially off-topic
content. In this case, the model must be extremely confident in what it marks
as "good ", but is more acceptable to mark "good" posts as off-topic. In an
equally valid scenario, the StackOverflow team may only want to remove
egregiously off-topic content and escalate borderline cases to a human
moderating team. In this case, the model must be extremely confident in what
it marks as "bad", but is more acceptable to allow "bad" posts through. Most
importantly, either scenario is valid and the best plan of action depends on
 the business needs.


# Balanced vs Imbalanced Classes
The provided dataset is perfectly balanced with 50% (50k) observations for both 
the positive (off-topic) and negative (acceptable) cases. This dataset detail
 allows for more lenient model metrics.
 
From the information provided, the cost of classifying a positive versus a
negative post is equal, which means that metrics such as accuracy are
acceptable. 
  
With imbalanced data sets, accuracy stops being as relevant of a metric (the
model can predict the majority class and get very good accuracy). In
addition, with imbalanced costs to mis-prediction, accuracy becomes less
relevant.
   
Because of these limitations, the pipeline reports accuracy and two metrics
(AUC and log_loss) that mitigate the imbalance and mis-prediction cost
limitations of accuracy.

# Model Edge Cases
Long posts (and titles)
- Many NLP algorithms (in the interest of memory size and processing speed)
look at only the first x words/characters of a document. In addition, the
 term frequency (the TF in TF-IDF) metric may be calculated using a log of
  term frequency (ln(TF)) because the difference between a term being present
   1 vs 3 times is much greater than a term being present 100 vs 300 times.
   
- This issue is somewhat mitigated by adding metadata regarding a document's
 length into the LogisticRegression classifier (in addition to the TF-IDF
  vectors)


Code syntax vs English syntax
- The training corpus of many NLP algorithms is the English language. Even
 though many computer languages use English words as syntax (for, in, while
 , return), the semantic interpretation of the same words may differ from
  standard usage in English. In addition, programming languages are
   significantly more structured in meaning than unstructured text
    descriptions. Therefore, algorithms may have difficulty distinguishing
     between word and sentence interpretations between text and code
     . In addition, code blocks may contain standard English within them in
      the form of comments. These comments are delineated by different
       characters (# in Python, // in SQL and C++) per language. Therefore to
        interpret comments as standard English, a pre-model should be trained
         to detect the code language first.
         
- This issue is somewhat mitigated by adding metadata regarding code comments
 into a classifier. Examples of code metadata may include the presence
 /absence of code, the length of code, the number of code snippets, and removing
  code specific stopwords (public, static, void, def, return) depending on
   the language detected.


# Generalizing the Model - New Off-Topic posts
Because the model was evaluated on data it was not trained on, it generalizes
 to new off-topic posts AS LONG AS the new off-topic posts match the sample
  of posts in the training data. For example, the criteria for labeling a
   post as off-topic should remain consistent. In addition, StackOverflow
    consists of multiple topics, and the language semantics may differ by topic.
   
When new topics are introduced into new posts or the subjects discussed depart
 from the historical StackOverflow corpus, the performance of the
  classification model will start to degrade. For example, users may adjust
   their behavior to avoid off-topic posts or realize that certain posts that
    are truly off-topic are bypassing the off-topic filter.
    
To avoid the topic drift challenge and continue to generalize to new off
-topic posts, the classification model (either TF-IDF, BERT, or another model
) should be periodically re-trained on a newly updated set of posts. In
 addition, the classifier model may choose to weight newer posts stronger to
  more dynamically adjust to the most recent data
 
# Generalizing the Model - New Reasons (Duplicates, Spam Posts)

## Duplicates
For duplicate posts, a cosine similarity metric of the TF-IDF/BERT vectors
would be used to classify similar posts. For posts that meet a desired
threshold of similarity, mark the mos recently posted post as a duplicate. 

## Spam
For spam posts, the model would likely perform better than random (spam may
 be considered a subset of off topic), but spam vs off-topic is a
 considerably different problem. Off-topic posts appear to be mainly one of
 3 categories (homework help/beginner questions, low effort posts without
 enough detail or examples, general programming topics) and are caused by
 carelessness or laziness of the users.
 
Spam may be considered to be more of an adversarial attack. Spam is targeted
, may be repeated, and may be actively hostile to the site's user base
 (phishing, unsolicited advertising, too frequent posts). As an adversarial
 attack, the forms of
 spam will likely change over time and may include specific tricks
 (intentional misspellings) that are intended to mimic but not strictly
 follow standard language protocols. As an adversarial method, the
 specific techniques used to detect spam will comprise of not only
 machine learning (which works best on historical data, the spammer's
 goal is to get past an automated filter. The filter would catch spam
 posts that is very similar to already caught spam posts)) but
 also techniques such as
 limiting new users, adding captchas to limit automated bots, user
 post flagging to moderators for further review, and banning users/IP
 Addresses that fail to follow site guidelines.
 
 
 
 # Reflections
 - If I did this assignment again, I would have not done anything
  significantly differently. I really appreciated the directions for being
   clean and concise, with a detailed objective. I also appreciated the
    explicit call out that the objective is NOT to create the best and most
     accurate model. I selected the TF-IDF model because it is quick to
      prototype and "good enough" for the time spent on the assignment. The
       likely "best" model for this question would be a transfer learning
        approach from a NN Transformer model
        such as BERT. With production grade systems (especially access to
         GPUs), I would select a pre-trained BERT model, fine tune the last
          few layers and embed the metadata features calculated into the NN
           architecture.
 
 # Dataset Issues
 - None noticed, very well structured dataset (almost too well structured
  compared to nearly all industry data sets)