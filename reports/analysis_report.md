# Overview and pipeline
The approach below details a classification pipeline to predict whether a
 particular post (title + body) on StackOverflow has been marked as off-topic
 . 
 
 The pipeline consists of 3 key stages:
 - Pre-processing
 - Model fitting
 - Model evaluation
  
# Preprocessing Overview
The preprocessing pipeline comprises of two key steps
 - Text parsing of the post's title and body
 - Calculating metadata features about the post's title and body 

The text parsing and cleaning portion consists of two stages:
## Light Cleaning
    - The light cleaning portion of the text parsing removes errata from the
     text but does adjust punctuation, create word lemmas, or remove stop words.
    - This version of the parsed text is used for the metadata details
     calculation and any NLP NN models that have been trained on embeddings
      for punctuation and for sequences of stop words
    - In particular, the BERT model from Google is trained on a corpus that
     includes such words and specifically uses them to understand sentence
     structure
    - Specific steps in light cleaning include:
        - Converting to lowercase -> capitalization is often a function of
         word placement in a sentence and represents the same word as its
          lower case counterpart
            - One nuance (not implemented, but may be useful with more
             analysis) may be to not convert UPPERCASE words as the represent
             acronyms distinct from the lower case counterpart (OR=operations
             Research vs or, ZIP as part of ZIP CODE vs zip as in the
             compressed file format)
        - Stripping HTML tags -> purely display information added by the
         website not the author
        - Converting URLs -> URLs are most often unique per post and per
         specific issue that the use is facing. URLs are often dynamically
         generated by the website, and are seldom repeated. However, the
         presence or absence of a link in a post is valuable and we should
         keep a dedicated word for URLs
        - Converting accents -> Diacritics in English (the language in the
         StackOverflow corpus) have a fairly marginal status. Words may be
         spelled with or without accents (cliche or naive) without changing
         their meaning
        - Expanding contractions -> Contractions in English contain the same
         information as their expanded counterparts. Many English writers
         will use the contraction and expanded form interchangeably.
        - Removing multiple spaces -> Multiple spaces contain no
         grammatical or interpretive meaning in English. In addition, the
         StackOverflow corpus contains significant amount of whitespace due
         to coding language requirements. It is important to tell the model
         that spaces in code are not indicative of any additional meaning
         beyond a syntactic requirement.
        - Considered converting emoji to a standard token because they are
         heavily person dependent on the exact emoji used. StackOverflow
         discourages emoji use on its platform so it is less critical to
         remove (low frequency in text). However, on a different set of text
         such as Twitter or SMS/WhatsApp/Instagram messages where emojis
         are more prevalent, would be useful to standardize them to an
         emoji token.
          
## Heavy Cleaning
    - The heavy cleaning portion of the text parsing standardizes individual
     words (more specifically tokens) into their base form and keeps only the
     essential portions of each text
    - This version of the parsed text is used for training models that cannot
     embed relationships across word families (TF-IDF, Bag of Words)
    - Specific steps in heavy cleaning include:
        - Removing punctuation: the presence / absence of punctuation is
         already captured in text metadata. In addition, multiple UTF-8 codes
          may represent the same character (single quote, double quote, comma
          ).
        - Lemmatizing text: multiple distinct words contain very similar
         meaning (stop, stopped, stops, stopping) and models should treat
         them as very similar in meaning. Converts all words to their base
          form (singular, present tense, non conjugated)
        - Removing stopwords: Words with high frequency of occurrence (and
        , is, I, she, it, or) often don't provide much additional meaning to
         a sentence so they should be removed from the text.

## Metadata
The calculated metadata features are intended to incorporate language based
information that is not necessarily the words themselves into the predictive
model. These features guide the model and help it explore non-token based
features of the language space.

The metadata features are calculated on the lightly cleaned (see above) post
 titles and bodies. The lightly cleaned version only removed errata from the
  text, but does not substantially alter the words and paragraphs themselves.
  
The following metadata features are computed:
- Number of code blocks in the post body
    - Posts with examples and code samples are more likely on topic
- Number of words in code blocks in the post body
    - Posts with shorter code samples are more likely on topic (if post
     has code, prefer concise examples, not long drawn out explanations)
- Number of sentences in the post body (sentence defined by punctuation)
    - Longer sentences may indicate more descriptive explanations
- Number of words in the title / body
    - More words in body correlate with more detailed descriptions, more
     descriptive error messages and behaviors and existing tried solutions
- Number of characters in the title / body
    - Similar explanation to number of words, but slightly differently encoded
    - Number of characters is controlled for URLs which are arbitrary distance
- Number of punctuation characters in the title / body
    - Posts with distinct punctuation (no run on sentences, proper commas) may
    be indicative of more thought and effort in the post
- Mean/Median/Max word length
    - Word lengths that are longer may correspond to more detailed and
     descriptive explanations
- Percent of words with meaning
    - After excluding stop words (am, on, it, etc.) from post, what percent
     of words are remaining. Posts with more "content" and less filler words
     may be indicative of detailed and specific code questions instead of
     generalities and vagueness

# Models

## Train / Test split

## TF-IDF

## Bert Transformer


# Model Evaluation Metrics
To evaluate model performance, I used a comprehensive set of evaluation
 metrics. The metrics are listed below:
 - ROC Curve (and AUC)
 - Accuracy
 - Log Loss

## TF-IDF Model Scores
For the TF-IDF model, the above mentioned scores are reported below:
- AUC:
- Accuracy: 
- Log-Loss: (predicting 0.5 for everything = 0.693)

In all 3 metrics, the TF-IDF model performs considerably better than random
and would provide valuable guidance to the poster regarding off-topic likelihood

## AUC
If I would pick a single metric, I would select AUC, which best handles
performance across a number of models. The AUC score represents the
probability that the classifier ranks a randomly chosen positive example
above a randomly chosen negative example. In addition, it generalizes to
the imbalanced and multi-class problems which are more realistic of the
true StackOverflow dataset. Submitted posts are not a 50/50 split between
good and bad, and there are multiple reasons a post may be considered bad
 (off-topic, spam, duplicate).

## Accuracy
However, AUC may be somewhat hard for a non-data scientist to intuitively
grasp (certainly possible but not immediately intuitive). To combat this
challenge, I report the accuracy score as well. Accuracy is a great metric
for this particular challenge because of the balanced classes in the
dataset. It measures what percentage of predictions the model gets correct
, which is the simplest metric of model performance. However, accuracy comes
 with a number of caveats for many data sets. With imbalanced classes
 , accuracy is not as useful of a metric
because a model can perform artificially well by predicting the majority
class for nearly all predictions. This phenomenon does not indicate true
 learning.
 
## AUC versus Accuracy
AUC mitigates this risk by using a "cutoff" score to score models. All
 results above the cutoff are predicted as a 1 and all results below the
 cutoff are predicted as a 0. The intermediate states of this graph will
 plot True Positive and False Positive scores, indicating how good the
 model is at discriminating between each class. Especially for models that
 predict on highly imbalanced data, comparing true and false positive
 rates will create a model that truly learns to distinguish the classes.
 
## Log Loss 
In addition to AUC and accuracy, I report log-loss as a way to distinguish how 
confident (or uncertain) the model is in its predictions. A model should be
 penalized for being confident about an incorrect prediction. In other words
 , a model should be rewarded for being more confident (scores closer to 1 or 0
 ) in its correct
predictions than hedging its predictions in the chance it may be wrong. It is
 better to be a little bit wrong about a prediction than completely wrong. If
the model is extremely confident that a post is good (class = 0), then it
should give a score close to 0. If the model is truly unsure of a particular
 post, 
then it should be encouraged to give a score of close to 0.5 rather than
randomly guessing and hoping the guess will be correct. 

## Business cost of False Positive vs False Negative
Finally, I would highlight that all three of the metrics above (AUC, Accuracy
, and Log Loss) are purely guiding principles for evaluating models. The best, 
and cost function will explicitly weigh the cost of mis-prediction in either
direction (predict good, actual bad vs predict bad, actual good) as defined
by the business objective. For example, the StackOverflow team may want to have 
an extra clean platform and preemptively remove any potentially off-topic
content. In this case, the model must be extremely confident in what it marks
as "good ", but is more acceptable to mark "good" posts as off-topic. In an
equally valid scenario, the StackOverflow team may only want to remove
egregiously off-topic content and escalate borderline cases to a human
moderating team. In this case, the model must be extremely confident in what
it marks as "bad", but is more acceptable to allow "bad" posts through. Most
importantly, either scenario is valid and the best plan of action depends on
 the business needs.


# Balanced vs Imbalanced Classes
The provided dataset is perfectly balanced with 50% (50k) observations for both the positive (off-topic) and negative (acceptable)

# Model Edge Cases
Long posts (and titles)
Code syntax vs English syntax


# Generalizing the Model - New Off-Topic posts
Because the model was evaluated on data it was not trained on, it should
 generalize
 ## TODO: discuss train-test splits here. Would work well for a different sample of posts with the same labeling scheme (AS LONG AS SO keeps the same criteria for labeling a post as off-topic (and ofc the data is in english)) Discuss topic drift
 
# Generalizing the Model - New Reasons (Duplicates, Spam Posts)

For duplicate posts, cosine similarity of the BERT vectors. Mark the most
 recently posted post a duplicate

For spam posts, the model would likely perform better than random (spam may
 be considered a subset of off topic), but spam vs off-topic is a
 considerably different problem. Off-topic posts appear to be mainly one of
 3 categories (homework help/beginner questions, low effort posts without
 enough detail or examples, general programming topics) and are caused by
 carelessness or laziness of the users.
 
Spam may be considered to be more of an adversarial attack. Spam is targeted
, may be repeated, and may be actively hostile to the site's user base
 (phishing, unsolicited advertising, too frequent posts). As an adversarial
 attack, the forms of
 spam will likely change over time and may include specific tricks
 (intentional misspellings) that are intended to mimic but not strictly
 follow standard language protocols. As an adversarial method, the
 specific techniques used to detect spam will comprise of not only
 machine learning (which works best on historical data, the spammer's
 goal is to get past an automated filter. The filter would catch spam
 posts that is very similar to already caught spam posts)) but
 also techniques such as
 limiting new users, adding captchas to limit automated bots, user
 post flagging to moderators for further review, and banning users/IP
 Addresses that fail to follow site guidelines.
 
 # Running the code
 - requirements.txt AND dockerfile
 
 ## CLI Interface
To run the pipeline, a CLI interface has been created in the
  okcupid_stackoverflow package in the `run.py` script.
  
The CLI interface takes 3 parameters. The parameters are the run_name (used
for distinguishing multiple model runs and providing a unique ID to save
particular models), the module (preprocessing or model fitting), and
use_metadata (which toggles the inclusion of metadata about the post in the
 model fitting step)
 
To run the code from the CLI interface, please use the following syntax from
the top level folder in the repository:
`python okcupid_stackoverflow/run.py --run_name=<INSERT RUN NAME> --module
=<INSERT MODULE> --use_metadata`

## Airflow Orchestration

#TODO: Insert short blurb about Airflow

To run the pipeline in a more automated fashion, an Airflow server has been
 set up at `INSERT IP ADDRESS HERE`. To trigger a particular run of the
  pipeline, please use the entry point found at `bin/airflow_trigger.sh`
  
For example, to trigger a manual run of the tf_idf pipeline: `bash bin
/airflow_trigger.sh -d tf_idf_pipeline -r test_123_must_be_unique -p "--use_metadata"`

This will run the pre-processing task first, and upon completion run the
 model_fitting and model_evaluation tasks. If any of the tasks fail, the
  Airflow server will send an email to the email address listed at the top of
   the DAG definition file (`dags/tf_idf_pipeline.py`)

## Serving Text Predictions with Flask 
- describe Flask for automated serving
 
 # Reflections
 - If I did this assignment again, I would have not done anything
  significantly differently. I really appreciated the directions for being
   clean and concise, with a detailed objective. I also appreciated the
    explicit call out that the objective is NOT to create the best and most
     accurate model. I selected the TF-IDF model because it is quick to
      prototype and "good enough" for the time spent on the assignment. The
       likely "best" model for this question would be a transfer learning
        approach from a NN Transformer model
        such as BERT. With production grade systems (especially access to
         GPUs), I would select a pre-trained BERT model, fine tune the last
          few layers and embed the metadata features calculated into the NN
           architecture.
 
 # Dataset Issues
 - None noticed, very well structured dataset (almost too well structured
  compared to nearly all industry data sets)