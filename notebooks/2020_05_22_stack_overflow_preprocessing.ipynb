{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import contractions\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from pandas.core.common import flatten\n",
    "from textstat import textstat\n",
    "import swifter\n",
    "\n",
    "\n",
    "def load_raw_data(data_path):\n",
    "    df = pd.read_csv(data_path, usecols=[\"Title\", \"Body\", \"label\"], encoding=\"utf-8\")\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_stopwords(language=\"english\"):\n",
    "    stopword_list = stopwords.words(language)\n",
    "    stopword_list.remove(\"no\")\n",
    "    stopword_list.remove(\"not\")\n",
    "    stopword_list.append(\"hi\")\n",
    "    stopword_list.append(\"please\")\n",
    "\n",
    "    return stopword_list\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    tag_regex = re.compile(r\"<[^>]+>\")\n",
    "    text = tag_regex.sub(\"\", text)\n",
    "\n",
    "    html_escape_table = {\n",
    "        \"&amp;\": \"and\",\n",
    "        \"&quot;\": '\"',\n",
    "        \"&apos;\": \"'\",\n",
    "        \"&gt;\": \">\",\n",
    "        \"&lt;\": \"<\",\n",
    "    }\n",
    "\n",
    "    for html_esc in html_escape_table:\n",
    "        text = text.replace(html_esc, html_escape_table[html_esc])\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def strip_urls(text):\n",
    "    # from geeksforgeeks.org/python-check-url-string\n",
    "    url_regex = re.compile(\n",
    "        r\"(?i)\\b((?:http?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)\"\n",
    "        r\"(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()\"\n",
    "        r\"<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    )\n",
    "    text = url_regex.sub(\"replacedurl\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def convert_accented_characters(text):\n",
    "    text = (\n",
    "        unicodedata.normalize(\"NFKD\", text)\n",
    "        .encode(\"ascii\", \"ignore\")\n",
    "        .decode(\"utf-8\", \"ignore\")\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text).lower()\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(\"[^A-Za-z0-9 ]+\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_text(text, nlp):\n",
    "\n",
    "    text = nlp(text)\n",
    "    text = \" \".join(\n",
    "        [word.lemma_ if word.lemma_ != \"-PRON-\" else word.text for word in text]\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_tokens = [token for token in text.split() if token not in load_stopwords()]\n",
    "    filtered_text = \" \".join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    text = text.replace(r\"\\n\", \"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def calc_num_sentences(text):\n",
    "    return textstat.sentence_count(text)\n",
    "\n",
    "\n",
    "def calc_num_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "def calc_num_chars(text):\n",
    "    return textstat.char_count(text, ignore_spaces=True)\n",
    "\n",
    "\n",
    "def calc_num_code_blocks(text):\n",
    "    return text.count(\"<code>\")\n",
    "\n",
    "\n",
    "def calc_num_punctuation_chars(text):\n",
    "    return len([x for x in text if x in set(string.punctuation)])\n",
    "\n",
    "\n",
    "def calc_num_words_in_code_blocks(text):\n",
    "    code_text = list(flatten(re.findall(r\"<code>(.*?)</code>\", text)))\n",
    "\n",
    "    n_words = 0\n",
    "    for snippet in code_text:\n",
    "        n_words += len(snippet.split())\n",
    "\n",
    "    return n_words\n",
    "\n",
    "\n",
    "def calc_word_len_mean(text):\n",
    "    words = text.split()\n",
    "    word_lengths = np.array([len(x) for x in words])\n",
    "\n",
    "    return np.mean(word_lengths)\n",
    "\n",
    "\n",
    "def calc_word_len_median(text):\n",
    "    words = text.split()\n",
    "    word_lengths = np.array([len(x) for x in words])\n",
    "\n",
    "    return np.median(word_lengths)\n",
    "\n",
    "\n",
    "def calc_word_len_max(text):\n",
    "    words = text.split()\n",
    "    word_lengths = np.array([len(x) for x in words])\n",
    "\n",
    "    return np.max(word_lengths)\n",
    "\n",
    "\n",
    "def normalize_text(doc, deep_clean=False, nlp=None):\n",
    "    # FOR BERT, Don't need to remove punctuation, don't need to remove stop-words\n",
    "    if not deep_clean:\n",
    "        doc = doc.lower()\n",
    "\n",
    "        doc = strip_html_tags(doc)\n",
    "        doc = strip_urls(doc)\n",
    "        doc = convert_accented_characters(doc)\n",
    "        doc = expand_contractions(doc)\n",
    "        doc = remove_multiple_spaces(doc)\n",
    "\n",
    "        # Tried to use the normalise library, but too slow\n",
    "        # Would use it for converting to/from numbers, percents, abbreviations\n",
    "\n",
    "        # Depending on text corpus, would want to process emojis as well\n",
    "        # However, SO is not very emoji friendly\n",
    "\n",
    "    else:\n",
    "        assert doc == doc.lower(), \"text has not been cleaned yet.\"\n",
    "        doc = remove_punctuation(doc)\n",
    "        doc = lemmatize_text(doc, nlp=nlp)\n",
    "        doc = remove_stopwords(doc)\n",
    "\n",
    "    return doc\n",
    "\n",
    "\n",
    "def run_preprocessing():\n",
    "    df = load_raw_data(\"../data/raw/interview_dataset.csv\")\n",
    "\n",
    "    # The removal of HTML tags will also remove the code block delimiter\n",
    "    df[\"num_code_blocks\"] = df[\"body\"].apply(calc_num_code_blocks)\n",
    "    df[\"num_words_code_blocks\"] = df[\"body\"].apply(calc_num_words_in_code_blocks)\n",
    "\n",
    "    df[\"light_cleaned_title\"] = (\n",
    "        df[\"title\"].swifter.allow_dask_on_strings().apply(normalize_text)\n",
    "    )\n",
    "    df[\"light_cleaned_body\"] = (\n",
    "        df[\"body\"].swifter.allow_dask_on_strings().apply(normalize_text)\n",
    "    )\n",
    "\n",
    "    df[\"light_cleaned_text\"] = (\n",
    "        df[\"light_cleaned_title\"] + \" \" + df[\"light_cleaned_body\"]\n",
    "    )\n",
    "\n",
    "    # Calculate meta-data features\n",
    "    df[\"num_sentences_body\"] = df[\"light_cleaned_body\"].apply(calc_num_sentences)\n",
    "    df[\"num_words_title\"] = df[\"light_cleaned_title\"].apply(calc_num_words)\n",
    "    df[\"num_words_body\"] = df[\"light_cleaned_body\"].apply(calc_num_words)\n",
    "    df[\"num_chars_title\"] = df[\"light_cleaned_title\"].apply(calc_num_chars)\n",
    "    df[\"num_chars_body\"] = df[\"light_cleaned_body\"].apply(calc_num_chars)\n",
    "    df[\"num_punctuation\"] = df[\"light_cleaned_body\"].apply(calc_num_punctuation_chars)\n",
    "    df[\"word_len_mean\"] = df[\"light_cleaned_body\"].apply(calc_word_len_mean)\n",
    "    df[\"word_len_median\"] = df[\"light_cleaned_body\"].apply(calc_word_len_median)\n",
    "    df[\"word_len_max\"] = df[\"light_cleaned_body\"].apply(calc_word_len_max)\n",
    "\n",
    "    nlp = en_core_web_sm.load()\n",
    "    # Normalize Text\n",
    "    df[\"cleaned_title\"] = (\n",
    "        df[\"light_cleaned_title\"]\n",
    "        .swifter.allow_dask_on_strings()\n",
    "        .apply(normalize_text, deep_clean=True, nlp=nlp)\n",
    "    )\n",
    "    df[\"cleaned_body\"] = (\n",
    "        df[\"light_cleaned_body\"]\n",
    "        .swifter.allow_dask_on_strings()\n",
    "        .apply(normalize_text, deep_clean=True, nlp=nlp)\n",
    "    )\n",
    "\n",
    "    df[\"cleaned_text\"] = df[\"cleaned_title\"] + \" \" + df[\"cleaned_body\"]\n",
    "    df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(remove_multiple_spaces)\n",
    "    df[\"num_words_body_cleaned\"] = df[\"cleaned_body\"].apply(calc_num_words)\n",
    "    df[\"pct_words_meaning\"] = df[\"num_words_body_cleaned\"] / df[\"num_words_body\"]\n",
    "\n",
    "    df.to_parquet(\"../data/processed/cleaned.parquet\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9911743815f4be1bd96ed73f1e9e90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dask Apply', max=8.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca6cd7f90ad48eb8c1fb86eb63dcc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dask Apply', max=8.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = run_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some EDA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n",
    "bins = 150\n",
    "\n",
    "for col, x in enumerate(df.dtypes):\n",
    "    if x != \"object\" and df.columns[col] != \"label\":\n",
    "        col_nm = df.columns[col]\n",
    "        plt.hist(df[df['label'] == 0][col_nm], alpha = 0.6, bins=bins, label='Relevant')\n",
    "        plt.hist(df[df['label'] == 1][col_nm], alpha = 0.8, bins=bins, label='Off_Topic')\n",
    "        plt.xlabel(col_nm)\n",
    "        plt.ylabel('Count of Posts')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xlim(0,df[col_nm].max())\n",
    "        plt.grid()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "corpus_0 = df[df.label == 0].cleaned_body.to_list()\n",
    "corpus_1 = df[df.label == 1].cleaned_body.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "word_cloud = WordCloud(\n",
    "                          background_color='black',\n",
    "                          max_font_size = 80\n",
    "                         ).generate(\" \".join(corpus_0))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "word_cloud = WordCloud(\n",
    "                          background_color='black',\n",
    "                          max_font_size = 80\n",
    "                         ).generate(\" \".join(corpus_1))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "okcupid_interview",
   "language": "python",
   "name": "okcupid_interview"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
